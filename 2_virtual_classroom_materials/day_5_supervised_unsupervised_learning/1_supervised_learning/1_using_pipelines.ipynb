{"cells":[{"cell_type":"markdown","source":["# Nice Pipeline\n\nhere we present a nice example of a pipeline which we can use for training purposes. At first glance, it looks messy and hard to read.  \nBut if you take a moment to understand, you will notice the beauty for sure!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fabe66d0-9253-4c70-a516-8aa650d0c647"}}},{"cell_type":"code","source":["from sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.compose import ColumnTransformer"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab587c62-f549-4b22-a0c3-818d5a130b54"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We just need to import some transformers which are inside of the pipeline.  \n**This is not a operational code, just an example on longer pipelines.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a35b6688-d268-425d-8539-b355d466862a"}}},{"cell_type":"code","source":["#Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#Dimensionality reduction\nfrom sklearn.decomposition import NMF\n\n#Imputation\nfrom sklearn.impute import SimpleImputer\n\n#Modeling\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Other\nimport numpy as np"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"004dfcda-d068-4e77-8334-d2ce0023b545"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Step 1: Take a quick glance\nPlease take a quick look onto the pipeline which is below and come back here.\n\n## Step 2: Slow walkthrough\nGet a **high level view** like this:\n- look toward the top, there is a *FeatureUnion*, which is really a wrapper for entire feature engineering\n- look at the bottom, there is a *RandomForestClassifier*, which is our predictive model\n\nNow we can go deeper inside of our FeatureUnion, which is our **feature engineering**:\n- it splits into three parts, depending on which features we are attempting to process\n    - on top, we have numerical features\n    - in the middle, we have categorical features\n    - on the bottom, we have textual features\n- now zoom out again and realize that this is wrapped under FeatureUnion, which means that these features will be transformed in a parallel way and appended next to each other\n\nOnly now let's **zoom into one part of our feature engineering**, for example into \"numerical features\", on the top:\n- inside of it, we right away need ColumnTransformer as we want to specify for which columns certain transformation will be applied by name or by type\n- now we could already be applying transformers, but remember that ColumnTransformer by default drops all untransformed columns, which would mean that if we want to apply some transformations sequentially we would not be able to\n\nFinally, **get used to the indentation** (the whitespacing). Your code editor helps with this. Get used to this by clicking just behind the last visible character on the line where you are. For example go behing the last bracket on the line of *SimpleImputer*. Now if you hit Enter, it will land where a code should continue on the next line it you still want to stay within the element, which is the *Pipeline*."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c74037c8-1b27-4d0d-9188-53185605bc72"}}},{"cell_type":"markdown","source":["Source1: https://www.codementor.io/@bruce3557/beautiful-machine-learning-pipeline-with-scikit-learn-uiqapbxuj\nSource2: http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22e0b88e-9865-4fd7-aaf6-8ae102d11b04"}}},{"cell_type":"code","source":["model_pipeline = Pipeline(steps=[\n    (\"features\", FeatureUnion([\n        (\"numerical_features\",\n         ColumnTransformer([\n             (\"numerical\",\n              Pipeline(steps=[(\n                  \"impute_stage\",\n                  SimpleImputer(missing_values=np.nan, strategy=\"median\")\n              )]),\n              [\"feature_1\"]\n             )\n         ])\n        ), \n        (\"categorical_features\",\n            ColumnTransformer([\n                (\"country_encoding\",\n                 Pipeline(steps=[\n                     (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n                     (\"reduction\", NMF(n_components=8)),\n                 ]),\n                 [\"country\"],\n                ),\n            ])\n        ), \n        (\"text_features\",\n         ColumnTransformer([\n             (\"title_vec\",\n              Pipeline(steps=[\n                  (\"tfidf\", TfidfVectorizer()),\n                  (\"reduction\", NMF(n_components=50)),\n              ]),\n              \"title\"\n             )\n         ])\n        )\n    ])\n    ),\n    (\"classifiers\", RandomForestClassifier())\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ba926a9-fe63-4e81-9d39-59e5680cb76e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now we would work with the pipeline easily:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"765332a2-33be-4c53-a65a-047894dfde88"}}},{"cell_type":"code","source":["#model_pipeline.fit(train_data, train_labels.values)\n#predictions = model_pipeline.predict(predict_data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bbf5b69d-af1f-4886-9267-ed5113f9a705"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# 3. How to write that?\nAlright, I now have a feeling that I am comfortable with understanding these, but how do we get to write such thing? The answer is: **from the outside - inwards**. Let's walk through an example, of course you could write things differently.  \n\nAt first, lay yourself a simple structure which separates your feature engineering (inside of FeatureUnion) and your predictive model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b20e254-4d49-45f6-89b1-8a0b7d0d4735"}}},{"cell_type":"code","source":["model_pipeline = Pipeline(steps=[\n    (\"features\", FeatureUnion([#all feature engineering goes here])),\n    (\"classifiers\", RandomForestClassifier())\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3acf7ce1-17d6-45f8-a917-7703a36752d0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;36m  File \u001B[0;32m\"<command-2649658360539189>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    model_pipeline = Pipeline(steps=[\u001B[0m\n\u001B[0m                              ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m expression cannot contain assignment, perhaps you meant \"==\"?\n","errorSummary":"<span class='ansi-red-fg'>SyntaxError</span>: expression cannot contain assignment, perhaps you meant \"==\"? (<command-2649658360539189>, line 1)","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;36m  File \u001B[0;32m\"<command-2649658360539189>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    model_pipeline = Pipeline(steps=[\u001B[0m\n\u001B[0m                              ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m expression cannot contain assignment, perhaps you meant \"==\"?\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Secondly, depending on your features, split yourself various parts inside of your feature engineering."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0eb2b1a3-a305-4ba5-aa99-bc4f5d965018"}}},{"cell_type":"code","source":["model_pipeline = Pipeline(steps=[\n    (\"features\", FeatureUnion([(\"numerical_features\", #numerical transformations), \n                               (\"categorical_features\", #categorical transformations), \n                               (\"text_features\", #textual transformations)\n                              ])\n    ),\n    (\"classifiers\", RandomForestClassifier())\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bde03802-df09-4f0a-8811-c5b57723d156"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;36m  File \u001B[0;32m\"<command-2649658360539204>\"\u001B[0;36m, line \u001B[0;32m5\u001B[0m\n\u001B[0;31m    ])\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m closing parenthesis ']' does not match opening parenthesis '(' on line 4\n","errorSummary":"<span class='ansi-red-fg'>SyntaxError</span>: closing parenthesis ']' does not match opening parenthesis '(' on line 4 (<command-2649658360539204>, line 5)","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;36m  File \u001B[0;32m\"<command-2649658360539204>\"\u001B[0;36m, line \u001B[0;32m5\u001B[0m\n\u001B[0;31m    ])\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m closing parenthesis ']' does not match opening parenthesis '(' on line 4\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now you want to put inside a ColumnTransformer as the transformations will be applied only to specific columns."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9127e34f-ce75-4a38-aaf3-f531fb0f791e"}}},{"cell_type":"code","source":["model_pipeline = Pipeline(steps=[\n    (\"features\", FeatureUnion([(\"numerical_features\", ColumnTransformer([#numerical transformations])),\n                               (\"categorical_features\", ColumnTransformer([#categorical transformations])),\n                               (\"text_features\", ColumnTransformer([#textual transformations]))\n                              ])\n    ),\n    (\"classifiers\", RandomForestClassifier())\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bec03745-6878-42da-a0d8-50d1f6d6b0bb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;36m  File \u001B[0;32m\"<command-2649658360539214>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    model_pipeline = Pipeline(steps=[\u001B[0m\n\u001B[0m                             ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m expression cannot contain assignment, perhaps you meant \"==\"?\n","errorSummary":"<span class='ansi-red-fg'>SyntaxError</span>: expression cannot contain assignment, perhaps you meant \"==\"? (<command-2649658360539214>, line 1)","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;36m  File \u001B[0;32m\"<command-2649658360539214>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    model_pipeline = Pipeline(steps=[\u001B[0m\n\u001B[0m                             ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m expression cannot contain assignment, perhaps you meant \"==\"?\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["You can put Pipeline inside of it, for example, in case you have transformers which need to be sequential (such as numeric scaling and feature selection).  \nAnd you just start to put in your individually wrote transformations from before."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b1a33d7-cdc2-49cb-82ed-4f269fc8c160"}}},{"cell_type":"markdown","source":["# 4. Reflect\nContinue with this point only once you went through the pipeline above.  \n\nUsually we think that nicely written code costs significantly more effort than code scraped together in whichever way. Now that we went through the composite estimators properly, you know that it might be even simpler in many cases, not to mention robustness.  \n\nYou are hopefully able to tell apart two things:  \n- Data preprocessing and wrangling.\n- Data preparation for ML (Feature Engineering)  \n\nAlways try to separate these things in your use case (code). That is why we present these topics separatedely. It will be of tremendous help in the longer run to write code in this way."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"123b6305-637e-48a0-8a28-c7b7f14155d1"}}},{"cell_type":"markdown","source":["# 5. Working Example  \n[Source](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b044a47-892f-4c47-acfd-86282343c1b3"}}},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6adccc8-a73a-40a7-b584-c409f48bc012"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["train = pd.read_csv(\"data_titanic/train.csv\")\ntrain.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"01282827-20f6-4e1f-856e-5d95dcf82460"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Use ``ColumnTransformer`` by selecting column by names\n\nWe will train our classifier with the following features:\n\nNumeric Features:\n\n* ``Age``: float;\n* ``Fare``: float.\n\nCategorical Features:\n\n* ``Embarked``: categories encoded as strings ``{'C', 'S', 'Q'}``;\n* ``Sex``: categories encoded as strings ``{'female', 'male'}``;\n* ``Pclass``: ordinal integers ``{1, 2, 3}``.\n\nWe create the preprocessing pipelines for both numeric and categorical data.\nNote that ``pclass`` could either be treated as a categorical or numeric\nfeature."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25d40f39-e009-499e-b13b-b01c79eba369"}}},{"cell_type":"code","source":["X = train.drop('Survived', axis=1)\ny = train['Survived']"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ac1a4beb-51b9-47cf-bd2d-09c0e75fb068"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["numeric_features = [\"Age\", \"Fare\"]\nnumeric_transformer = Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), \n                                      (\"scaler\", StandardScaler())]\n                              )\n\ncategorical_features = [\"Embarked\", \"Sex\", \"Pclass\"]\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n\npreprocessor = ColumnTransformer(transformers=[(\"num\", numeric_transformer, numeric_features),\n                                               (\"cat\", categorical_transformer, categorical_features),\n                                              ]\n                                )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2b21501-668a-44d4-b0ef-d35c43497bb8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Append classifier to preprocessing pipeline. Now we have a full prediction pipeline."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8696f73-230c-494d-8003-f23f0a17f339"}}},{"cell_type":"code","source":["clf = Pipeline(steps=[(\"preprocessor\", preprocessor), \n                      (\"classifier\", LogisticRegression())])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nclf.fit(X_train, y_train)\n\nprint(\"model score: %.3f\" % clf.score(X_test, y_test))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7969788d-154c-4170-b659-e0a68799966f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"model score: 0.799\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["model score: 0.799\n"]}}],"execution_count":0},{"cell_type":"code","source":["clf"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f299cec-4e32-47a2-9cef-01b88171a9b8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[55]: Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Age', 'Fare']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['Embarked', 'Sex',\n                                                   'Pclass'])])),\n                ('classifier', LogisticRegression())])","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[55]: Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['Age', 'Fare']),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['Embarked', 'Sex',\n                                                   'Pclass'])])),\n                ('classifier', LogisticRegression())])"]}}],"execution_count":0},{"cell_type":"markdown","source":["Use ``ColumnTransformer`` by selecting column by data types\n\nWhen dealing with a cleaned dataset, the preprocessing can be automatic by\nusing the data types of the column to decide whether to treat a column as a\nnumerical or categorical feature.\n\n`sklearn.compose.make_column_selector` gives this possibility.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>In practice, you will have to handle yourself the column data type.\n   If you want some columns to be considered as `category`, you will have to\n   convert them into categorical columns. If you are using pandas, you can\n   refer to their documentation regarding [Categorical data](https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html).</p></div>\n\n\n+ First, we will transform the object columns into categorical.  \n+ Then, let's only select a subset of columns to simplify our example."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f9cb4ed-30f3-42aa-9c30-218bd628a5ea"}}},{"cell_type":"code","source":["X[\"Embarked\"] = X[\"Embarked\"].astype(\"category\")\nX[\"Sex\"] = X[\"Sex\"].astype(\"category\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3920fd9a-295e-4db6-a44a-51c683d5b2b1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nsubset_feature = [\"Embarked\", \"Sex\", \"Pclass\", \"Age\", \"Fare\"]\nX_train, X_test = X_train[subset_feature], X_test[subset_feature]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfefc458-f27e-426c-984f-72ee1f40aa59"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["X_train.info()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a69dc129-2118-435b-9d27-d82e57f16dbf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 712 entries, 140 to 684\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   Embarked  710 non-null    category\n 1   Sex       712 non-null    category\n 2   Pclass    712 non-null    int64   \n 3   Age       571 non-null    float64 \n 4   Fare      712 non-null    float64 \ndtypes: category(2), float64(2), int64(1)\nmemory usage: 23.9 KB\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["<class 'pandas.core.frame.DataFrame'>\nInt64Index: 712 entries, 140 to 684\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   Embarked  710 non-null    category\n 1   Sex       712 non-null    category\n 2   Pclass    712 non-null    int64   \n 3   Age       571 non-null    float64 \n 4   Fare      712 non-null    float64 \ndtypes: category(2), float64(2), int64(1)\nmemory usage: 23.9 KB\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["We can observe that the `embarked` and `sex` columns were tagged as `category` columns.  \nTherefore, we can use this information to dispatch the categorical columns to the ``categorical_transformer`` and the remaining columns to the ``numerical_transformer``."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"444d8b82-4dae-4eaf-85a3-94fa8e5309f2"}}},{"cell_type":"code","source":["from sklearn.compose import make_column_selector as selector\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n        (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n    ]\n)\nclf = Pipeline(\n    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n)\n\n\nclf.fit(X_train, y_train)\nprint(\"model score: %.3f\" % clf.score(X_test, y_test))\nclf"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b61ce65-1d50-4a2e-82f1-9fc091e25b5d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"model score: 0.799\nOut[59]: Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x7f4707d67fa0>),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x7f4707d67670>)])),\n                ('classifier', LogisticRegression())])","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["model score: 0.799\nOut[59]: Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(strategy='median')),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x7f4707d67fa0>),\n                                                 ('cat',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x7f4707d67670>)])),\n                ('classifier', LogisticRegression())])"]}}],"execution_count":0},{"cell_type":"markdown","source":["The resulting score is not exactly the same as the one from the previous\npipeline because the dtype-based selector treats the ``pclass`` column as\na numeric feature instead of a categorical feature as previously:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e428227f-adfb-462a-be1d-f6b5d11098fb"}}},{"cell_type":"code","source":["selector(dtype_exclude=\"category\")(X_train)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5216343e-8ee4-45a8-9911-35d506da0c82"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[60]: ['Pclass', 'Age', 'Fare']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[60]: ['Pclass', 'Age', 'Fare']"]}}],"execution_count":0},{"cell_type":"code","source":["selector(dtype_include=\"category\")(X_train)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27b7aea6-342b-4754-8647-fbf6b975eef6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[61]: ['Embarked', 'Sex']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[61]: ['Embarked', 'Sex']"]}}],"execution_count":0},{"cell_type":"markdown","source":["Using the prediction pipeline in a grid search  \n\nGrid search can also be performed on the different preprocessing steps defined in the ``ColumnTransformer`` object, together with the classifier's\nhyperparameters as part of the ``Pipeline``.  \nWe will search for both the imputer strategy of the numeric preprocessing and the regularization parameter of the logistic regression using\n:class:`~sklearn.model_selection.GridSearchCV`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57fc6d5b-e0c3-4dea-bbbd-d56bb4ddafb4"}}},{"cell_type":"code","source":["param_grid = {\"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"],\n              \"classifier__C\": [0.1, 1.0, 10, 100],\n             }\n\ngrid_search = GridSearchCV(clf, param_grid, cv=10)\ngrid_search"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29ed8dfd-864b-4589-8076-c30828cf5ea7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[62]: GridSearchCV(cv=10,\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         <sklearn.compose._column_transformer.make_column_selector object at 0x7f4707d67fa0>),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         <sklearn.compose._column_transformer.make_column_selector object at 0x7f4707d67670>)])),\n                                       ('classifier', LogisticRegression())]),\n             param_grid={'classifier__C': [0.1, 1.0, 10, 100],\n                         'preprocessor__num__imputer__strategy': ['mean',\n                                                                  'median']})","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[62]: GridSearchCV(cv=10,\n             estimator=Pipeline(steps=[('preprocessor',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('imputer',\n                                                                                          SimpleImputer(strategy='median')),\n                                                                                         ('scaler',\n                                                                                          StandardScaler())]),\n                                                                         <sklearn.compose._column_transformer.make_column_selector object at 0x7f4707d67fa0>),\n                                                                        ('cat',\n                                                                         OneHotEncoder(handle_unknown='ignore'),\n                                                                         <sklearn.compose._column_transformer.make_column_selector object at 0x7f4707d67670>)])),\n                                       ('classifier', LogisticRegression())]),\n             param_grid={'classifier__C': [0.1, 1.0, 10, 100],\n                         'preprocessor__num__imputer__strategy': ['mean',\n                                                                  'median']})"]}}],"execution_count":0},{"cell_type":"markdown","source":["Calling 'fit' triggers the cross-validated search for the best hyper-parameters combination:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6730a8d6-7b33-47ae-a88f-61c25f699d60"}}},{"cell_type":"code","source":["grid_search.fit(X_train, y_train)\n\nprint(\"Best params:\")\nprint(grid_search.best_params_)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"429b6833-0782-4741-a17e-ec2b97ab2730"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Best params:\n{'classifier__C': 0.1, 'preprocessor__num__imputer__strategy': 'median'}\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Best params:\n{'classifier__C': 0.1, 'preprocessor__num__imputer__strategy': 'median'}\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["The internal cross-validation scores obtained by those parameters is:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9fedb7cf-c157-4202-b0dc-403ea9f3589a"}}},{"cell_type":"code","source":["print(f\"Internal CV score: {grid_search.best_score_:.3f}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4aefabe6-00db-4118-b774-09bdace562e4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Internal CV score: 0.788\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Internal CV score: 0.788\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["We can also introspect the top grid search results as a pandas dataframe:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ababfbc9-9302-480e-9d15-a400f68d20d8"}}},{"cell_type":"code","source":["cv_results = pd.DataFrame(grid_search.cv_results_)\ncv_results = cv_results.sort_values(\"mean_test_score\", ascending=False)\ncv_results[[\"mean_test_score\",\n            \"std_test_score\",\n            \"param_preprocessor__num__imputer__strategy\",\n            \"param_classifier__C\",\n           ]].head(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2605b91d-3124-4378-ae76-2a59561a914b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>param_preprocessor__num__imputer__strategy</th>\n      <th>param_classifier__C</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.788009</td>\n      <td>0.040220</td>\n      <td>median</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.786600</td>\n      <td>0.039922</td>\n      <td>mean</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.785211</td>\n      <td>0.039990</td>\n      <td>mean</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.785211</td>\n      <td>0.039491</td>\n      <td>median</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.785211</td>\n      <td>0.039990</td>\n      <td>mean</td>\n      <td>10</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>param_preprocessor__num__imputer__strategy</th>\n      <th>param_classifier__C</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.788009</td>\n      <td>0.040220</td>\n      <td>median</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.786600</td>\n      <td>0.039922</td>\n      <td>mean</td>\n      <td>0.1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.785211</td>\n      <td>0.039990</td>\n      <td>mean</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.785211</td>\n      <td>0.039491</td>\n      <td>median</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.785211</td>\n      <td>0.039990</td>\n      <td>mean</td>\n      <td>10</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["The best hyper-parameters have be used to re-fit a final model on the full training set.  \nWe can evaluate that final model on held out test data that was not used for hyperparameter tuning."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"abe85821-4527-4514-a3ed-e509e11c1aa5"}}},{"cell_type":"code","source":["print(f\"best logistic regression from grid search: {grid_search.score(X_test, y_test):.3f}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb2cfa9b-6d79-435a-b183-e43c9404cc1d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"best logistic regression from grid search: 0.799\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["best logistic regression from grid search: 0.799\n"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"1_using_pipelines","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2649658360538974}},"nbformat":4,"nbformat_minor":0}
